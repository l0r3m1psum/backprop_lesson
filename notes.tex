\documentclass{article}

\title{Back-propagation Lecture Notes}
\author{Diego Bellani}
\date{2025}

\usepackage{matrixcalculus}

\begin{document}

\maketitle

\section{Inroduction}

When training a neural network it is necessary to calculate the gradient of the
loss w.r.t. the parameters. Automating this process is necessary since manual
differentiation is time consuming and error prone.

A possible way to automate it is to is \emph{numerical differentiation}. That
is, in its most basic form, approximating the gradient using the definition of
the derivative, i.e. \[\lim_{h \to 0} \frac{f(x+h)-f(x)} h.\]

Let us try to see if this is a viable option with an example, let
\begin{equation}
f(x_1, x_2) = \log(x_1)+x_1x_2-\sin(x_2) \label{eqn:example}
\end{equation}
and say that we want to calculate \(\nabla{f(2,5)}\). We want to confront the
value that we would obtain from the numerical approximation against the formula
that we would find by hand. Since \(\nabla{f(2,5)}\) has two elements, let us
start from the second. We know from basic calculus that
\(\partialfrac{f(x_1,x_2)}{x_2} = x_1-\cos(x2)\). Now to use the limit
definition in our numerical approximation, since computers cannot take limits,
we are going to use a small number instead for \(h\). Let us write a quick
implementation below.

\begin{verbatim}
import numpy
def f(x1,x2): return numpy.log(x1) + x1*x2 - numpy.sin(x2)
x1, x2, h = numpy.array([2.,5.,1e-5], dtype=numpy.float32)
d_f_x2 = (f(x1,x2+h) - f(x1,x2))/h
print(numpy.absolute((x1 - numpy.cos(x2)) - d_f_x2))
\end{verbatim}

The code above prints \texttt{0.09509146}, this means that the numerical
approximation, in this simple example, is already off by one decimal point.
This is not acceptable, not to mention that to find a gradient of a function
with \(n\) arguments it requires \(O(n)\) evaluations of the function, which may
be very expensive.

Another way to automate the calculation of the gradient is to use
\emph{symbolic differentiation}. That is to treat the expression of the loss
as an data structure\footnote{A directed acyclic graph.} and write an algorithm
that applies the rules of calculus for us. Some people may tell you that this
approach does not scale due to \emph{expression swell}, that is to say that the
na\"ive application of calculus rules can lead to an explosion in the size of
the expression and in a lot of redundant calculations. E.g.
\(\partialfrac{\sin(e^x)} x = e^x \cos(e^x)\), as you can see in a na\"ive
implementation we have to evaluate and store the node representing \(e^x\) two
times. This problem can be solved with \emph{common sub-expression
elimination}. The real problem lies in the fact that we have to use another
language to specify our neural network and in how to differentiate through
control-flow.\footnote{Again this is not strictly true. We could use our own
programming language as the language to specify the expression that we want to
differentiate and than use \emph{reflection} to transform it in a suitable data
structure. With this approach we can also make control-flow structures work but
if you get this far you have to make some philosophy about what you mean by
symbolic differentiation\dots~\cite{equiv}.}

In the rest of this document we are going to introduce the
\emph{back-propagation} algorithm which is a special case of \emph{reverse-mode
automatic differentiation}, which is one of the differentiation modes studied
by the field of \emph{automatic differentiation}~\cite{survey,griewank}.

\section{Scalar Back-propagation}

To recap, we need a an efficient way to calculate gradient, at \emph{machine
precision}, of a function written in a programming language, supporting all of
its control-flow constructs. This task may seem daunting but we will show how
it can be done, starting from the general idea, then describing how it is
usually implemented and how this gives us for free support for deriving through
control-flow.

Let us start by considering equation~\ref{eqn:example}, in particular how we can
use the chain rule to calculate the two elements of its gradient. Let us start
by decomposing equation~\ref{eqn:example} in its elementary operations, as
shown of the left side of figure~\ref{fig:example}. We can then express the two
elements of the gradient as
\begin{eqnarray*}
\partialfrac{f(x_1,x_2)}{x_1}
&=& \partialfrac{v_5}{v_4}\left(\partialfrac{v_4}{v_1}\partialfrac{v_1}{v_{-1}}
	+ \partialfrac{v_4}{v_2}\partialfrac{v_2}{v_{-1}}\right) \\
&=& \partialfrac{v_5}{v_4}\partialfrac{v_4}{v_1}\partialfrac{v_1}{v_{-1}}
	+ \partialfrac{v_5}{v_4}\partialfrac{v_4}{v_2}\partialfrac{v_2}{v_{-1}} \\
\partialfrac{f(x_1,x_2)}{x_2}
&=& \partialfrac{v_5}{v_3}\partialfrac{v_3}{v_0}
	+ \partialfrac{v_5}{v_4}\partialfrac{v_4}{v_2}\partialfrac{v_2}{v_0}
\end{eqnarray*}
and in figure~\ref{fig:dag} we have the dependencies among the variables used
to evaluate the expression.

\begin{figure}
\begin{minipage}{.5\textwidth}
\begin{eqnarray*}
v_{-1} &=& x_1 \\
v_0 &=& x_2 \\
v_1 &=& \log(v_{-1}) \\
v_2 &=& v_{-1}v_0 \\
v_3 &=& \sin(v_0) \\
v_4 &=& v_1+v_2 \\
v_5 &=& v_4-v_3 \\
f(x_1,x_2) &=& v_5,
\end{eqnarray*}
\end{minipage}
\begin{minipage}{.5\textwidth}
\makepartialfractextstyle
\begin{eqnarray*}
\bar v_5 &=& 1 \\
\bar v_4 &=& \bar v_5 \partialfrac{v_5}{v_4} \\
\bar v_3 &=& \bar v_5 \partialfrac{v_5}{v_3} \\
\bar v_1 &=& \bar v_4 \partialfrac{v_4}{v_1} \\
\bar v_2 &=& \bar v_4 \partialfrac{v_4}{v_2} \\
\bar v_0 &=& \bar v_3 \partialfrac{v_3}{v_0} \\
\bar v_{-1} &=& \bar v_2 \partialfrac{v_2}{v_{-1}} \\
\bar v_0 &\pluseq& \bar v_2 \partialfrac{v_2}{v_0} \\
\bar v_{-1} &\pluseq& \bar v_1 \partialfrac{v_1}{v_{-1}}
\end{eqnarray*}
\end{minipage}
\caption{Left forward propagation. Right back-propagation of the gradient.}
\label{fig:example}
\end{figure}

\begin{figure}
\centering
\includegraphics{figures.1}
\caption{Graph representing the expression.}
\label{fig:dag}
\end{figure}

We can now start to note a few things. The paths in the graph from the source
node \(v_5\) to the sinks \(v_{-1}\) and \(v_0\) correspond to the addends in
the chain rule, e.g. \(\partialfrac{v_5}{v_3}\partialfrac{v_3}{v_0}\)
corresponds to the path \(((v_5,v_3), (v_3,v_0))\). We also have that we can
evaluate a path (or addend of a chain rule), from left to right, e.g. \[
\partialfrac{v_5}{v_4}\partialfrac{v_4}{v_1}\partialfrac{v_1}{v_{-1}}
= \partialfrac{v_5}{v_1}\partialfrac{v_1}{v_{-1}}
= \partialfrac{v_5}{v_{-1}},\]
that is we can evaluate it by left-multiplying the derivatives of \(v_5\)
w.r.t. every intermediate variable \(v_i\) along the path.

This are the fundamental ideas behind the back-propagation algorithm. We
can calculate the gradient incrementally, starting at the source node and going
backward, storing at each node \(v_i\) its derivative with respect to the
source \(\partialfrac{v_5}{v_i}\). If we let \(\bar v_i =
\partialfrac{v_5}{v_i}\) we can see how this calculation is performed on the
right side of figure~\ref{fig:example}. Note how for each intermediate variable
\(v_i\) the calculation takes always the following form
\(\bar v_i = \bar v_j \partialfrac{v_j}{v_i}\) where \(v_j\) is a node that
depends on \(v_i\); we call this operation, for reasons which are going to be
clear later, the vector-Jacobian product (vJp). The back-propagation algorithm
is shown in figure~\ref{fig:algorithm}.

\begin{figure}
\centering
\begin{minipage}{5cm}
\begin{tabbing}
\(s \leftarrow \max V\)\\
\(\bar v_s \leftarrow 1\)\\
for\=\ \(j \in V\) in topological order\\
\> for\=\ \(i \in \{i : (j, i) \in E\}\)\\
\> \> \(\displaystyle\bar v_i \leftarrow \bar v_i + \bar v_j \partialfrac{v_j}{v_i}\)
\end{tabbing}
\end{minipage}
\caption{The back-propagation algorithm, where the \(G = (V, E)\) is the DAG
representing the scalar function.}
\label{fig:algorithm}
\end{figure}

The way in which this is usually implemented in a programming language is by
keeping track of all the variables and operations used on them so that at the
end of the computation of the function we have a graph that we can use to
back-propagate the gradient through. In the case of Python this is done by
creating a class like the one below.

\begin{verbatim}
import dataclasses, enum
class Operation(enum.Enum): ...
@dataclasses.dataclass
class Scalar:
    value: float
    gradient: float
    operation: Operation
    parent1: Scalar
    parent2: Scalar
\end{verbatim}

To make the generation of the graph more ergonomic one can overload the standard
mathematical operation on \texttt{Scalar} objects. It is important that before
the back-propagation step the graph is topologically sorted.

We will illustrate the way in which this enables us to back-propagate through
contol-flow with an example. When you train a neural network you update its
weights in a loop, so if your network is represented by a static expression the
graph that is used to back propagate has always the same form,\footnote {Of
course the values stored in it are going to change.} if in the code that we use
to specify the neural network we have an \texttt{if} that take one of its
branches based on a calculated value the graph that is going to be generated is
going to differ from one of the iteration of the loop to another. E.g. \texttt
{x = a+b if a > 0 else a+c}. The back-propagation algorithm is therefore going
to propagate the gradient though a different graph every time but it does not
care about how it was generated, if by an \texttt{if}, a \texttt{for} or an
exception.

\section{Back-propagation for Vectors and Matrices}

We now have an algorithm that it is capable of differentiating any code, saddly
this is not enough for our needs\dots There are two main problems with this
approach. We are now going to illustrate them first for the case in which the
gradient is back-propagated through vector operations then for the more general
case of matrix operations.

The first one is a memory problem, to train neural networks we often do
computations with big matrices, multiplying two matrices of size \(n \times n\)
requires at least \(\Omega(n^2)\) mathematical operations.\footnote{The
naive matrix multiplication algorithm requires \(O(n^3)\) operations, there are
better ones but they cannot require less than \(\Omega(n^2)\), so we are going
to make a very rosey assumption here.} If we consider that a single precision
floating point number occupies 4 bytes, and that to keep track of an operation
we are going to need at least 17 bytes\footnote{For a binary operation say we
allocate 4 bytes for each pointer to the 2 parents, 4 bytes for the value of the
operation and another 4 for its gradient, and 1 byte to distinguish a among the
various kinds of operations.} so there are 9 bytes of overhead per operation.
If we let \(n=1024\) we would be \(9 \cdot 1024^2\), i.e. 9MiB, of overhead for
just a single operation.

The second problem is one of performance, if the values of our operations all
around in memory it not possible to use optimized BLAS routines to implement
the linear algebra operations we care about, also due to how computers work it
is not really possible to implement them ourselves if the data is not stored
densely in memory. Since what we care about are mostly matrix operations it
makes sense to perform operations directly on vectors (and matrices) and not
perform them at the more granular scalar level. we will need to add a few more
bytes to keep track of the dimension of the vector/matrix but it is going to be
worth it.

Doing this introduces another problem. To see what it is let us start with a
quick refresh of multivariable calculus let \(\vec y = \vec f(\vec x)\), where
\(\vec f\) is a function of vector argument and value. Its derivative is a
matrix called the Jacobian with the following contents \[
\left[\begin{array}{ccc}
\partialfrac{y_1}{x_1} & \cdots & \partialfrac{y_m}{x_1} \\
\vdots & \ddots & \vdots \\
\partialfrac{y_1}{x_n} & \cdots & \partialfrac{y_m}{x_n} \\
\end{array}\right]\!.
\] So back propagating the gradient now requires multiplying the gradient
vector \(\vec g\) with the Jacobian at that node.

The problem is going to become evident with the following example, let
\(\vec f(\vec x) = \vec a \hadam \vec x,\) where \(\hadam\) is the element-wise
product. Its Jacobian is \[
J = \left[\begin{array}{ccc}
a_1 & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & a_n \\
\end{array}\right]\!,
\] that is a diagonal matrix with \(\vec a\) as its diagonal. Multiplying the
gradient with this matrix is the same as doing an element-wise multiplication,
that is \[
J \vec g = \vec a \hadam \vec g.
\] This is because \(J\) is \emph{sparse}, i.e. most of its elements are 0.
We can therefore implement matrix vector multiplication in a more space and
time efficient way by just storing \(\vec a\) instead of \(J\). This is called
Jacobian-vector product (vJp) and can often be calculated implicitly in an
efficient way.

It is important to note that not all Jacobians are sparse, if we let \(\vec
f(\vec x) = A \vec x,\) where A is a matrix, then
\(\partialfrac{\vec f(\vec x)}{\vec x} = A'\), where \(A'\) is \(A\)
transposed.

So the Jacobians of many functions we care about are sparse and we can make some
space and time savings by being smart in the way in which we store and operate
on this sparse Jacobians.

Neural networks are often trained in batches so the multiplications performed
with matrices are not in the form of a matrix and a vector but of a matrix and
a matrix and a matrix. Let \(F(X) = AX,\) what is \(\partialfrac{F(X)}{X}\)?

The answer is: its debated~\cite{notion}. Why is this important? Matrix
multiplication is arguably the most important operation in neural networks,
therefore it is very important to be able to back propagate through it. Where
is the problem? Whatever \(\partialfrac{F(X)}{X}\) is it needs to contain al
possible partial derivatives of the input w.r.t. the output, for simplicity say
that the input and and the output have the same shape and are both square so
\(\partialfrac{F(X)}{X}\) needs to contain \(n \times n \times n \times n\)
numbers in it; if \(n=1024\) then \(\partialfrac{F(X)}{X},\) for single
precision floating point numbers, needs to occupy at least 4TiB, which as of
today no GPU on earth can store. Luckily it is sparse, we just need a way to
implicitly perform the vJp.

We are going to need a few pieces, first let \(\langle\vec x,\vec y\rangle\) be
the inner product of the vectors \(\vec x\) and \(\vec y.\) It can be proven
that \(\partialfrac{\langle\vec f(\vec x),\vec g(\vec x)\rangle}{\vec x}
= \partialfrac{\vec f(\vec x)}{\vec x} \vec g(\vec x)
	+ \partialfrac{\vec g(\vec x)}{\vec x} \vec f(\vec x)\), so if we keep one
of the two arguments constant deriving the inner product gives us the vJp. E.g.
\(\partialfrac{\langle\vec a \hadam \vec x,\vec g\rangle}{\vec x}
= \vec a \hadam \vec g.\)

Let \(\vect(A)\) be a vector made by stacking the columns of the matrix \(A\)
one on top of the other. In this way we can generalize our our definition of
inner product to matrices by letting \(\langle A,B\rangle =
\langle\vect(A),\vect(B)\rangle\). Let \(\trace(A)\) be the sum of the diagonal
entries of a square matrix, we have that \(\langle A,B\rangle = \trace(A'B)\).
Finally it can be proven that \(\partialfrac{\trace(X'A)}{X} =
A\)~\cite{cookbook,magnus}.

We are now ready to find the vJp of the matrix-matrix multiplication \(A X\),
let \(G\) be the matrix containing the gradients of the batch in its columns,
then: \[
\partialfrac{\langle AX,G\rangle}{X}
= \partialfrac{\trace((AX)'G)}{X}
= \partialfrac{\trace(X'A'G)}{X}
= A'G.\]

% TODO add examples from
%  * https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b
%  * https://cs231n.stanford.edu/slides/2016/winter1516_lecture4.pdf
%  * https://cs231n.stanford.edu/slides/2024/lecture_4.pdf#page=103
% TODO add questions from the exam...

\subsection{An Alternative Derivation}

% https://www.brown.edu/Departments/Engineering/Courses/En221/Notes/Index_notation/Index_notation.htm
% https://robotchinwag.com/posts/einsum-gradient/
% Chapter 26 of Mathematical Methods for Physics an Engineering
% TODO: add sensible definition of free and bound indices.

We are now going to derive the same result as above but using ``index
notation''. Let us start by writing the expression \(\vec c = A \vec b\) using an
explicit summation as
\begin{equation}
	c_i = \sum_j a_{ij} b_j. \label{eq:sum}
\end{equation}
We are going to introduce the concept of \emph{free} and \emph{bounded} indices
that will make summation symbol superfluous in~\ref{eq:sum}. A free index is an
index that appears only once in a summation and not otherwise defined while a
bounded index is an indexed that appears more than once. Now in~\ref{eq:sum}
\(i\) is a free and \(j\) is bounded. By adopting as a convention that in an
expression involving products of indexed variables the bounded indices are the
ones summed over we can rewrite \ref{eq:sum} as \(c_j = a_{ij} b_{j}\). This
convention is known as the Einstein notation.

This allows us to write any \emph{contraction}, unambiguously, without the need
for summation symbols, where a contraction is an expression that involves a sum
of products of a set of indexed variables. For example:

\noindent
\begin{minipage}{.5\textwidth}
\begin{eqnarray*}
A \vec x &\rightarrow& a_{ij} x_j, \\
A B &\rightarrow& a_{ij} b_{jk}, \\
A' B &\rightarrow& a_{ji} b_{jk}, \\
\vec y \vec x' &\rightarrow& y_i x_j,\\
\end{eqnarray*}
\end{minipage}
\begin{minipage}{.5\textwidth}
\begin{eqnarray*}
\vec y A \vec x' &\rightarrow& y_i a_{ij} x_j, \\
\trace(A) &\rightarrow& A_{ii}, \\
\vec y' \vec x &\rightarrow& y_i x_i, \\
\vec z = \vec y \hadam x &\rightarrow& z_i = y_i x_i.\\
\end{eqnarray*}
\end{minipage}

Note how, in the last example we had to write an equality, because the
expression otherwise would have been equal to the one above. Putting an index on
the left of the equality clears the meaning and it is why we use
the ``not otherwise defined'' clause in the definition above. This problem could
be solved by using \emph{contra} and \emph{contravariant} indices but always
writing the expression in an equality avoids the need to introduce this new
concepts and this is what we are going to do from now on.

We now want to define a calculus over this contractions but first we need a
small extension to our convention: the Kronecker delta, which is defined as
\marginpar{Sometimes \(\delta_{ij}\) is written as \(e_i \krone e_j\) where
\(\krone\) in this context is not to be interpreted as the Kronecker product in
the matrix sense but its own thing.}
\[
\delta_{ij} = \left\{\begin{array}{ll}
                     1 & \mbox{if \(i = j\)}; \\
                     0 & \mbox{otherwise}.
                     \end{array}\right.
\] This symbol has the following properties:
\begin{itemize}
\item \(\delta_{ij} = \delta_{ji}\) from the commutativity of the equality
between indices;
\item \(\delta_{ij} a_j = a_i\) and \(\delta_{ij} a_{jk} = a_{ik}\) from the
transitivity of the equality between indices.
\end{itemize}

We can now define define a simple calculus for contractions by noting that
\(\partialfrac{a_i}{a_j} = \delta_{ij}\) and
\(\partialfrac{a_{ij}}{a_{kl}} = \delta_{ik}\delta_{jl}\). The first result
follows from noting that \(\partialfrac{\vec a}{\vec a} = I\). We are now ready
to write the derivative of a product between to matrices
\(c_{ij} = f_{ij}(x) = a_{ik} x_{kj}\) as \[
\partialfrac{}{x_{\alpha\beta}} f_{ij}(x)
= \partialfrac{}{x_{\alpha\beta}} a_{ik} x_{kj}
= a_{ik} \partialfrac{x_{kj}}{x_{\alpha\beta}}
= a_{ik} \delta_{k\alpha} \delta_{j\beta}
= a_{i\alpha} \delta_{j\beta}.
\] Note how the result is an object with four indices i.e.
\(\left(\partialfrac f x\right)_{ij\alpha\beta}\). It is necessary to add this
two new \emph{fresh} indices, denoted with Greek letters, because we need the
rate of change of every variable w.r.t. every other. Since we are using indices
the expression is composed only by multiplications between scalars, hence, with
the equalities noted above, differentiation is trivial.

Deriving the formula for gradient propagation is also trivial since if we let
\(l\) be our loss we have that \[
\partialfrac{l}{x_{\alpha\beta}}
= \partialfrac{l}{c_{ij}} \partialfrac{c_{ij}}{x_{\alpha\beta}}
= \partialfrac{l}{c_{ij}} a_{i\alpha} \delta_{j\beta}
= \partialfrac{l}{c_{ij}} \delta_{j\beta} a_{i\alpha}
= \partialfrac{l}{c_{i\beta}} a_{i\alpha}
= a_{i\alpha} \partialfrac{l}{c_{i\beta}},
\] which is just \(A' G\) where \(G\) is \(\partialfrac{l}{c_{i\beta}}\).

As you can see, once you get used to it, the index notation is more expressive
and easier to use than the classic matrix one in this context. It also
generalizes better to higher dimensions (or more indices). This also makes
automatic differentiation libraries based on this formalism more expressive,
see~\cite{laue}. This convention is also implemented in common software
libraries e.g. NumpPy implements it with the function
\texttt{numpy.einsum}.\footnote{This functions usually take as input the two
variables and a string describing the contraction. For some incomprehensible
reason the syntax used translating this expressions like
\(c_{ij} = a_{ik} b_{kj}\) is \texttt{ik,kj->ij} instead of something like
\texttt{ij=ik*kj}.}

% \(a_i\stackrel{i}{*}b_i\) a better notation that Laue et ali

% https://stackoverflow.com/a/47609896
% TODO: parlare di einsum e di come implementare la retropopagazione per il suo
% caso generale i.e. n indici
% è possibile trasformare una contrazione nella trasformazione lineare
% (matriciale) equivalente? E.g. \delta_{ij} a_{kl} = I \knone A

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
